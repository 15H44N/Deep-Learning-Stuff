{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import torch #main torch module\n",
    "import torch.nn as nn #neural net module\n",
    "import torch.optim as optim #optimizers\n",
    "import torch.nn.functional as F #functions like ReLu Sig Tanh etc\n",
    "from torch.utils.data import DataLoader #help us with datasets\n",
    "\n",
    "import torchvision.datasets as datasets #using to access std data\n",
    "import torchvision.transforms as transforms #transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Network Definition\n",
    "We will define our RNN here.  \n",
    "Now we will use the RNN with Images.  \n",
    "Image has 28x28x1 dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "We dont use RNNs for Images, but in this case we can imagine  \n",
    "as there are 28 time steps in this RNN, and we will send the   image row by row into the RNN. Each row is 28 that will go  \n",
    "So we will use, different hyperparameters, as given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "input_size = 28\n",
    "sequence_length = 28\n",
    "num_layers = 2\n",
    "hidden_size = 256\n",
    "num_classes = 10\n",
    "learning_rate =  0.001\n",
    "batch_size = 64\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Now we will define RNN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size, hidden_size, \n",
    "                          num_layers, batch_first = True)\n",
    "        #No of sequences is not necessary, works for any number\n",
    "        #batch first\n",
    "        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h0 = torch.zeros(self.num_layers, \n",
    "                         x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        #forward prop\n",
    "        out, _ = self.rnn(x,h0)\n",
    "        out = out.reshape(out.shape[0],-1)\n",
    "        out = self.fc(out)\n",
    "        #keeping batch as first axis, flatten rest\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class RNN_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN_LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, \n",
    "                          num_layers, batch_first = True)\n",
    "        #No of sequences is not necessary, works for any number\n",
    "        #batch first\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h0 = torch.zeros(self.num_layers, \n",
    "                         x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        #extra cell state required for LSTM\n",
    "        c0 = torch.zeros(self.num_layers, \n",
    "                         x.size(0), self.hidden_size).to(device)\n",
    "        #forward prop\n",
    "        out, _ = self.lstm(x,(h0,c0))\n",
    "        \n",
    "        out = self.fc(out[:,-1,:])\n",
    "#       Here we only take the last hidden state as it has info from\n",
    "#         all prev\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Modelling and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Set Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "train_dataset = datasets.MNIST(root='dataset/',\n",
    "                               train=True, \n",
    "                               transform = transforms.ToTensor(),\n",
    "                               download = True)\n",
    "\n",
    "train_loader = DataLoader(dataset = train_dataset,\n",
    "                         batch_size = batch_size,\n",
    "                         shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Loading Test Data\n",
    "test_dataset = datasets.MNIST(root='dataset/',\n",
    "                               train=False, \n",
    "                               transform = transforms.ToTensor(),\n",
    "                               download = True)\n",
    "\n",
    "test_loader = DataLoader(dataset = test_dataset,\n",
    "                         batch_size = batch_size,\n",
    "                         shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#init the network\n",
    "model = RNN_LSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx,(data, targets) in enumerate(train_loader):\n",
    "        # data to devices\n",
    "        data = data.to(device).squeeze(1) #squeeze 64x1x28x28 to 64x28x28\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        #fwd\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        \n",
    "        #back\n",
    "        optimizer.zero_grad() #so that it does not store prev backprop calc\n",
    "        loss.backward()\n",
    "        \n",
    "        #gradient desc\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def check_accuracy(loader,model):\n",
    "    if loader.dataset.train:\n",
    "        print(\"Checking Training Data Accuracy\")\n",
    "    else:\n",
    "        print(\"Checking Test Data Accuract\")\n",
    "    \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() #set to evaluation mode\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #only have to check accuracy, dont compute grads\n",
    "        for x,y in loader:\n",
    "            x = x.to(device).squeeze(1)\n",
    "            y = y.to(device)\n",
    "            #x = x.reshape(x.shape[0],-1)\n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            \n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "            \n",
    "        accuracy = float(num_correct)/float(num_samples)*100\n",
    "        print(f\"Got {num_correct} / {num_samples} with accuracy {accuracy: .2f}\")\n",
    "\n",
    "    model.train()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Test Data Accuract\n",
      "Got 9699 / 10000 with accuracy  96.99\n",
      "Checking Training Data Accuracy\n",
      "Got 58130 / 60000 with accuracy  96.88\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "96.88333333333333"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_accuracy(test_loader,model)\n",
    "check_accuracy(train_loader,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch-gpu]",
   "language": "python",
   "name": "conda-env-torch-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
